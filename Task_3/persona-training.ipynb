{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101034, 2)\n",
      "                                               Input  \\\n",
      "0  I would love to try the local food with my fri...   \n",
      "1  I would love to try the local food with my fri...   \n",
      "2  I would love to try the local food with my fri...   \n",
      "3  I would love to try the local food with my fri...   \n",
      "4  I would love to try the local food with my fri...   \n",
      "\n",
      "                                              Output  \n",
      "0                                 What's your name?   \n",
      "1   Nice to meet you Gavin. What kind of movies d...  \n",
      "2   I can relate to that. I like to watch movies ...  \n",
      "3            What are some of your favorite movies?   \n",
      "4   Those are all great movies! I love \"The Shaws...  \n",
      "Train dataset length: 80\n",
      "Validation dataset length: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./tokenizer/tokenizer_config.json',\n",
       " './tokenizer/special_tokens_map.json',\n",
       " './tokenizer/vocab.json',\n",
       " './tokenizer/merges.txt',\n",
       " './tokenizer/added_tokens.json',\n",
       " './tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Loading the dataset\n",
    "data = pd.read_csv('output.csv')\n",
    "print(data.shape)\n",
    "print(data.head())\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos token for consistency\n",
    "\n",
    "# Prepare data for training\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "def tokenize(text_in, text_out):\n",
    "    inputs = tokenizer(text_in, padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=256)\n",
    "    labels = tokenizer(text_out, padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=256)[\"input_ids\"]\n",
    "    \n",
    "    X.append({\n",
    "        'input_ids': inputs[\"input_ids\"].squeeze(0),\n",
    "        'attention_mask': inputs[\"attention_mask\"].squeeze(0)\n",
    "    })\n",
    "    Y.append(labels.squeeze(0))\n",
    "\n",
    "# Tokenizing the data\n",
    "for index, row in data.iterrows():\n",
    "    tokenize(row['Input'], row['Output'])\n",
    "    if len(X) == 100 and len(Y) == 100:  \n",
    "        break  \n",
    "\n",
    "# Create the dataset\n",
    "dataset = Dataset.from_dict({\n",
    "    \"input_ids\": [x[\"input_ids\"].tolist() for x in X],  # No need to clone, just convert to list\n",
    "    \"attention_mask\": [x[\"attention_mask\"].tolist() for x in X],\n",
    "    \"labels\": [y.tolist() for y in Y]\n",
    "})\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, shuffle=True)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# Check dataset lengths\n",
    "print(f\"Train dataset length: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset length: {len(val_dataset)}\")\n",
    "\n",
    "# Convert to DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = torch.stack(batch['input_ids']).to(model.device)\n",
    "        attention_mask = torch.stack(batch['attention_mask']).to(model.device)\n",
    "        labels = torch.stack(batch['labels']).to(model.device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss every 10 steps\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Save the model weights\n",
    "torch.save(model.state_dict(), \"model_weights.pth\")\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(\"./tokenizer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6491555,
     "sourceId": 10484606,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "dsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
